{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain import OpenAI, VectorDBQA\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "#import magic\n",
    "#import os\n",
    "#import nltk\n",
    "import config\n",
    "\n",
    "# os.environ['OPENAI_API_KEY'] = '...'\n",
    "\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# pip install unstructured\n",
    "# Other dependencies to install https://langchain.readthedocs.io/en/latest/modules/document_loaders/examples/unstructured_file.html\n",
    "# pip install python-magic-bin\n",
    "# pip install chromadb\n",
    "# pip install unstructured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DirectoryLoader('doc/', glob='*.pdf')\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce the dimensionality of the embeddings to 2D using PCA\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "reduced_vectors = pca.fit_transform(vectors)\n",
    "\n",
    "# Visualize the embeddings in a scatter plot\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.scatter(reduced_vectors[:, 0], reduced_vectors[:, 1])\n",
    "\n",
    "# Add labels to the scatter plot\n",
    "for i, label in enumerate(labels):\n",
    "    plt.annotate(label, (reduced_vectors[i, 0], reduced_vectors[i, 1]))\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Chroma using direct local API.\n",
      "Using DuckDB in-memory for database. Data will be transient.\n",
      "Exiting: Cleaning up .chroma directory\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' The articles written by Abonia are \"https://arxiv.org/abs/2212.11804\" and \"https://arxiv.org/ftp/arxiv/papers/2006/2006.02767.pdf\".'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "texts = text_splitter.split_documents(documents)\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=config.OPENAI_API_KEY)\n",
    "docsearch = Chroma.from_documents(texts, embeddings)\n",
    "\n",
    "qa = VectorDBQA.from_chain_type(llm=OpenAI(openai_api_key = config.OPENAI_API_KEY), chain_type=\"stuff\", vectorstore=docsearch)\n",
    "\n",
    "query = \"What are the name of the artciles written  by  Abonia?\"\n",
    "qa.run(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Google certified professional data engineer, IBM Quantum Conversation Badge, Machine learning specialization par deeplearning.IA, Andrew NG, Deep learning & natural language specialization par deeplearning.IA, Andrew NG, Spark par learning Academy, Advance Python bootcamp, GCP professional data engineer Badges, Watson Assistant Hands on, Méthodologie Watson Assistant, Méthodologie Watson Fondation, Watson Knowledge Studio Hands on.'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa = VectorDBQA.from_chain_type(llm=OpenAI(openai_api_key = config.OPENAI_API_KEY), chain_type=\"stuff\", vectorstore=docsearch)\n",
    "\n",
    "query = \"Give the list of certifications in the CV\"\n",
    "qa.run(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' The names of the Medium articles written by Abonia are \"How I Passed the GCP Professional Data Engineer Exam\", \"Get Trello Boards Data With Py-Trello\", \"PyCaret for Machine Learning\", and \"Machine Learning in Google Cloud With BigQuery\".'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa = VectorDBQA.from_chain_type(llm=OpenAI(openai_api_key = config.OPENAI_API_KEY), chain_type=\"stuff\", vectorstore=docsearch)\n",
    "query = \"What are the name of the medium artciles written  by  Abonia?\"\n",
    "qa.run(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "detectron2 is not installed. Cannot use the hi_res partitioning strategy. Falling back to partitioning with the fast strategy.\n",
      "detectron2 is not installed. Cannot use the hi_res partitioning strategy. Falling back to partitioning with the fast strategy.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Chroma using direct local API.\n",
      "Using DuckDB in-memory for database. Data will be transient.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nThe articles written by Abonia are: \\n- How I passed the GCP Professional Data Engineer Exam\\n- Get Trello Boards data with py-trello\\n- PyCaret for Machine Learning\\n- Machine Learning in Google Cloud with BigQuery\\n\\nI hope this answers your question. Is there anything else I can help you with?'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.prompts import PromptTemplate\n",
    "import config\n",
    "from langchain import OpenAI,VectorDBQA\n",
    "\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain import OpenAI, VectorDBQA\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "#import magic\n",
    "import os\n",
    "import nltk\n",
    "import config\n",
    "\n",
    "loader = DirectoryLoader('doc/', glob='*.pdf')\n",
    "documents = loader.load()\n",
    "\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "texts = text_splitter.split_documents(documents)\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=config.OPENAI_API_KEY)\n",
    "docsearch = Chroma.from_documents(texts, embeddings)\n",
    "\n",
    "prompt_template = PromptTemplate(template=config.prompt_template, input_variables=[\"context\", \"question\"])\n",
    "doc_chain = load_qa_chain(\n",
    "    llm=OpenAI(\n",
    "        openai_api_key = config.OPENAI_API_KEY,\n",
    "        model_name=\"text-davinci-003\",\n",
    "        temperature=0,\n",
    "        max_tokens=300,\n",
    "    ),\n",
    "    chain_type=\"stuff\",\n",
    "    prompt=prompt_template,\n",
    ")\n",
    "qa = VectorDBQA(vectorstore=docsearch, combine_documents_chain=doc_chain, k=config.k)\n",
    "query = \"What are the name of the medium artciles written  by  Abonia?\"\n",
    "qa.run(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "qa = VectorDBQA.from_chain_type(llm=OpenAI(openai_api_key = config.OPENAI_API_KEY), chain_type=\"stuff\", vectorstore=docsearch, return_source_documents=True)\n",
    "query = \"What are the name of the artciles written  by  Abonia?\"\n",
    "result = qa({\"query\": query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Abonia Sojasingarayar\\n\\nMachine Learning Scientist | Data Scientist | NLP Engineer | Computer Vision Engineer | AI\\n\\nAnalyst | Technical Writer\\n\\nAbonia SOJASINGARAYAR\\n\\nData scientist - Machine Learning\\n\\nEngineer – Natural Language\\n\\nProcessing et Computer Vision\\n\\nEngineer\\n\\nFormation et Qualification\\n\\n&Education\\n\\nFormation\\n\\n2021 : M2 - Intelligence Artificielle\\n\\nIA School – Boulogne-Billancourt\\n\\n2020 : M1 - Chef De Projet Digital\\n\\nInstitut F2I – Paris (France)\\n\\n2015 : Licence en technologie informatique et Ingénierie\\n\\nUniversité Pondicherry – Karikal (Inde)\\n\\nCertification et\\n\\nBadge\\n\\nGoogle certified professional data engineer\\n\\nIBM Quantum Conversation Badge\\n\\nMachine learning specialization par deeplearning.IA , Andrew NG\\n\\nDeep learning & natural language specialization par deeplearning.IA,Andrew NG\\n\\nSpark par learning Academy\\n\\nAdvance Python bootcamp\\n\\nGCP professional data engineer Badges\\n\\nWatson Assistant Hands on\\n\\nMéthodologie Watson Assistant\\n\\nMéthodologie Watson Fondation', lookup_str='', metadata={'source': '/Users/sojasingarayar/Documents/Projet_perso/Context-Based-LLMChatbot/doc/CV.pdf'}, lookup_index=0),\n",
       " Document(page_content=\"https://arxiv.org/ftp/arxiv/papers/2006/2006.02767.pdf\\n\\nAbonia SOJASINGARAYAR                                                                      01/2023\\n\\nAbonia Sojasingarayar\\n\\nMachine Learning Scientist | Data Scientist | NLP Engineer | Computer Vision Engineer | AI\\n\\nAnalyst | Technical Writer\\n\\n&\\n\\nExpérience professionnelle\\n\\nProfil : Je suis passionnée par l'intelligence artificielle, le deep learning, l'apprentissage automatique, le\\n\\ntraitement du langage naturel et la vision par ordinateur. J'ai précédemment travaillé dans les domaines\\n\\ndes solutions de gestion RH, l'industrie chimique et la cyber sécurité. Ma mission personnelle est de créer\\n\\ndes solutions basées sur l'IA qui résolvent des problèmes à fort impact pour les gens du monde entier et\\n\\nsimplifient la vie quotidienne.\\n\\nCompétences fonctionnelles\\n\\nDistribution (Ventes- E-Commerce etc…)\\n\\nIndustrie (Chimique, IT, Solution RH, Cyber sécurité)\\n\\nCompétences techniques\\n\\nMéthodes et Outils →Développent en mode Agile\", lookup_str='', metadata={'source': '/Users/sojasingarayar/Documents/Projet_perso/Context-Based-LLMChatbot/doc/CV.pdf'}, lookup_index=0),\n",
       " Document(page_content='Watson Assistant Hands on\\n\\nMéthodologie Watson Knowledge Studio\\n\\nWatson Knowledge Studio Hands on\\n\\nLangues\\n\\nFrançais Intermédiaire\\n\\nAnglais Bilingue\\n\\nTamoul Maternelle\\n\\nHindi Bilingue\\n\\nMedium Writer\\n\\nhttps://medium.com/@abonia\\n\\nLinkedin &\\n\\nPublications\\n\\nLinkedin : https://www.linkedin.com/in/aboniasojasingarayar/\\n\\nGithub : https://github.com/Abonia1\\n\\nGoogle Scholar:\\n\\nhttps://arxiv.org/abs/2212.11804\\n\\nhttps://arxiv.org/ftp/arxiv/papers/2006/2006.02767.pdf\\n\\nAbonia SOJASINGARAYAR                                                                      01/2023\\n\\nAbonia Sojasingarayar\\n\\nMachine Learning Scientist | Data Scientist | NLP Engineer | Computer Vision Engineer | AI\\n\\nAnalyst | Technical Writer\\n\\nAbonia SOJASINGARAYAR\\n\\nData scientist - Machine Learning\\n\\nEngineer – Natural Language\\n\\nProcessing et Computer Vision\\n\\nEngineer\\n\\nFormation et Qualification\\n\\n&Education\\n\\nFormation\\n\\n2021 : M2 - Intelligence Artificielle\\n\\nIA School – Boulogne-Billancourt\\n\\n2020 : M1 - Chef De Projet Digital', lookup_str='', metadata={'source': '/Users/sojasingarayar/Documents/Projet_perso/Context-Based-LLMChatbot/doc/CV.pdf'}, lookup_index=0),\n",
       " Document(page_content=\"Machine Learning Scientist | Data Scientist | NLP Engineer | Computer Vision Engineer | AI\\n\\nAnalyst | Technical Writer\\n\\nRésolution des problèmes de sécurité et de sauvegarde des données.\\n\\nClonage de disque dur et configuration des paramètres du disque dur\\n\\nOutils et techniques : Clone de disque dur, maintien du poste PC, réparer\\n\\nl'imprimante…\\n\\nAbonia SOJASINGARAYAR                                                                      01/2023\\n\\nAbonia Sojasingarayar\\n\\nMachine Learning Scientist | Data Scientist | NLP Engineer | Computer Vision Engineer | AI\\n\\nAnalyst | Technical Writer\\n\\nRésolution des problèmes de sécurité et de sauvegarde des données.\\n\\nClonage de disque dur et configuration des paramètres du disque dur\\n\\nOutils et techniques : Clone de disque dur, maintien du poste PC, réparer\\n\\nl'imprimante…\\n\\nAbonia SOJASINGARAYAR                                                                      01/2023\", lookup_str='', metadata={'source': '/Users/sojasingarayar/Documents/Projet_perso/Context-Based-LLMChatbot/doc/CV.pdf'}, lookup_index=0)]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['source_documents']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain import OpenAI, VectorDBQA\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "import os\n",
    "import nltk\n",
    "import config\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    handlers=[\n",
    "        logging.FileHandler(config.LOGS_FILE),\n",
    "        logging.StreamHandler(),\n",
    "    ],\n",
    ")\n",
    "LOGGER = logging.getLogger(__name__)\n",
    "\n",
    "loader = DirectoryLoader(config.FILE_DIR, glob='*.pdf')\n",
    "documents = loader.load()\n",
    "\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "texts = text_splitter.split_documents(documents)\n",
    "\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=config.OPENAI_API_KEY)\n",
    "docsearch = Chroma.from_documents(texts, embeddings)\n",
    "\n",
    "# Define answer generation function\n",
    "def answer(prompt: str, persist_directory: str = config.PERSIST_DIR) -> str:\n",
    "    LOGGER.info(f\"Start answering based on prompt: {prompt}.\")\n",
    "    #vectorstore = Chroma(persist_directory=persist_directory, embedding_function=embeddings)\n",
    "    prompt_template = PromptTemplate(template=config.prompt_template, input_variables=[\"context\", \"question\"])\n",
    "    doc_chain = load_qa_chain(\n",
    "        llm=OpenAI(\n",
    "            openai_api_key = config.OPENAI_API_KEY,\n",
    "            model_name=\"text-davinci-003\",\n",
    "            temperature=0,\n",
    "            max_tokens=300,\n",
    "        ),\n",
    "        chain_type=\"stuff\",\n",
    "        prompt=prompt_template,\n",
    "    )\n",
    "    LOGGER.info(f\"The top {config.k} chunks are considered to answer the user's query.\")\n",
    "    qa = VectorDBQA(vectorstore=docsearch, combine_documents_chain=doc_chain, k=config.k)\n",
    "    #qa = VectorDBQA.from_chain_type(llm=OpenAI(openai_api_key = config.OPENAI_API_KEY), chain_type=\"stuff\", vectorstore=docsearch)\n",
    "    result = qa({\"query\": prompt})\n",
    "    answer = result[\"result\"]\n",
    "    LOGGER.info(f\"The returned answer is: {answer}\")\n",
    "    LOGGER.info(f\"Answering module over.\")\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer(\"What are the name of the artciles written  by  Abonia?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fc686abb362e3d8df54a0cb2384c8a3d97cbf9b946071814b06d7190502c631d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
